
# Software Architecture

As mentioned previously, the ADC software developed for the original ExoCube was not rigorously tested and was not designed to be reused on future PolySat missions. This work set out to design the ADC software so that it is easy to reuse and extend on future missions, compatible with the larger PolySat code base, and testable in the most flight-like configuration possible. Although the control algorithm for ExoCube 2 is beyond the scope of this paper, its place in the software architecture is discussed for the sake of completeness.

Although high-level languages and environments like MATLAB, Simulink, and Python provide robust ecosystems for developing and validating ADC algorithms, PolySat spacecraft are barred from flying these tools due to the highly resource constrained PolySat flight computer. For this reason, the ADC software was written in C to minimize memory footprint and maximize computational efficiency. This choice had the added benefit of being compatible with the wider PolySat code base [@greg].

## Reusability and Extensibility

Reusability and extensibility were achieved by implementing a basic model of object orientation in C [@ooc]. More specifically, we defined several generic interfaces that together accomplish all the features of the ADC software. A high-level diagram of some these interfaces and their interactions is shown in Figure \ref{arch}. Each block represents a different interface. In this UML-style diagram, the arrows do not represent inheritance, but rather reflect the dependency hierarchy within the software. In other words, `ADCS State` stores instances of `Mission Determination` and `Mission Controller`, `Mission Controller` contains multiple `Controller` objects, etc. Because these blocks represent interfaces, different implementations can be easily swapped in and out, enabling ADC developers to reuse past implementations and provide new implementations with little effort. A configuration file based plugin system was developed to simplify this implementation swapping. Note that the software utilizes many more generic interfaces than depicted, but they were left out of the diagram to reduce clutter.

The two branches depicted in the diagram represent the two primary components of the software, determination and then control. The yellow arrows at the bottom represent the flow of data within the software---the attitude estimate created by the determination side is passed to the control side, and the control torques generated by the control side are passed back to the determination side to be used in the predict step of the Kalman filter.

![Object model for the ADC software architecture.\label{arch}](paper/img/arch.pdf){ width=60% }

### Control Architecture

To create a complete picture of the software architecture, all of the interfaces will be described from the bottom of Figure \ref{arch} upward, starting on the control side. The lowest level interface is the `Actuator`, which is a lightweight API abstracting the actuator drivers away (e.g. magnetorquer and reaction wheel drivers). Users can pass torque or magnetic dipole moment vectors to these objects and expect the appropriate physical action to take place. Note that the `Actuator` interface actually refers to a set of interfaces, as the API to generically control reaction wheels is fundamentally different from that of magnetorquers.

Next we move to the `Controller` interface, which utilizes the `Actuator` interfaces to control the attitude or angular velocity of the spacecraft. The `Mission Controller` object passes the `Controller` an attitude estimate, relevant sensor readings, other data that could be useful within a control law, and potentially a desired orientation or angular velocity for the spacecraft. The `Controller` object is where the actual control law is implemented. For example, there is an implementation of a `BDOT Controller`, which torques the magnetorquers in the direction opposite that of the change in magnetic field to slow down the spacecraft's angular velocity. One could also imagine implementing a `Reaction Wheel Controller`, which utilizes reaction wheels and some control law to torque the spacecraft into a desired orientation. The torques and dipole moments generated by the `Controller` is passed back to the determination branch so that they can be included in the Kalman filter predict step.

The `Mission Controller` is responsible for switching between different `Controller`'s as dictated by the mission requirements. For example, if a spacecraft first needs to detumble using BDOT before starting its primary control law, the `Mission Controller` would run the `BDOT Controller` until some convergence criteria was met, and then initialize the primary control law. The `Controller`'s themselves are thus agnostic to these mission-specific mode changes.

The larger ADC code base is responsible for calling methods within the `Mission Controller` at the appropriate time. The `Controller` and further the `Mission Controller` are expected to return the amount of time that should elapse before being called again. This allows the `Controller` to vary its time interval. For example, within the `BDOT Controller`, the algorithm samples the magnetometers twice to compute the change in magnetic field, and then proceeds to torque the magnetorquers in the appropriate manner. The time interval between magnetometer samples may be different than the torque duration, thus the `BDOT Controller` needs to vary the time between calls. The `Controller` also returns a flag indicating whether the determination algorithm should be run or not. This temporary suspension of the determination algorithm has many uses. For example, in magnetorquer-dependent `Controller`'s like the `BDOT Controller`, the determination algorithm should not be run while the magnetorquers saturate the magnetometers.

### Determination Architecture

The lowest level of the determination branch is the `Magnetic Model` and `Sun Model`. These are interfaces that take the spacecraft's position and the current time and produce ECI magnetic and solar reference vectors respectively. These are needed by the `Filter` class, which is where the Kalman filter algorithm described above resides, to perform the update step. Wrapping solar and magnetic models in a generic interface allows ADC developers to utilize the reference model that best satisfies the mission requirements.

Implementations of the `Filter` interface are passed sensor readings and control torques and are expected to produce an attitude estimate. The `Mission Filter` adds an extra degree of mission-specific flexibility on top of the `Filter` by allowing missions to switch between `Filter` implementations if needed. Furthermore, if the mission needs to model disturbance torques that are not modeled in generic `Filter` implementations, the `Filter` interface provides an API to allow `Mission Determination` implementations to provide these contributions to the dynamics.

As mentioned previously, the determination code is executed at the command of the control algorithm. It has no mechanism to change the rate at which it is called.

### Configuration Files

This software utilizes a robust network of configuration files to allow specifying which `Filter`, `Controller`, `Actuator`, etc. implementations to use, thus building out a basic plugin or extension system. As this code base matures, and more robust `Filter` and `Controller` algorithms are prototyped and tested, mission developers will be able to support full attitude determination and control by writing just a series of configuration files and bare-bones implementations of the `Mission Determination` and `Mission Controller` interfaces, which should be similar to example implementations. This enables developers to maximize code reuse and implement only the ADC functionality that is specific to the mission, minimizing risk and development time.

## Testing and Verification

Since it is impossible to fully recreate orbital conditions on earth, and even crude approximations are expensive, validating the performance of ADC software can be a challenge, especially for an undergraduate research lab with a small budget. For this reason, a software framework was developed so that all hardware interactions could be virtualized and modeled using a spacecraft dynamics simulator. The high-level architecture of this framework is shown in Figure \ref{sim}.

![ADC software testing framework architecture.\label{sim}](paper/img/sim.png){ width=60% }

The core of the hardware virtualization happens within `libpolydrivers`, PolySat's hardware abstraction library used across its flight software. `libpolydrivers` abstracts all sensor and actuator hardware to a consistent interface. Thus, drivers were implemented for every sensor and actuator used by the ADC software so that, instead of interacting with physical hardware, the driver sends a JSON packet to a simulation server where the appropriate action is taken. For example, if the ADC software was attempting to interact with a magnetometer, the simulation server would respond with the current magnetic field estimate in the body frame for the given position. If the software was attempting to interact with a reaction wheel, the simulation server would model the induced torque.

Because the hardware virtualization happens within the driver abstraction layer, the user must only change a configuration value to switch into simulation mode. No changes to the ADC software are required. This means that the actual flight ADC software build can be tested without any changes---all the logic to perform the simulation happens within the driver library, which is a dynamic library loaded at runtime. Furthermore, because the virtualization is network based (it sends a JSON request to an external server), the ADC software can be running on the resource-constrained flight hardware while the simulation server runs on an external computer, significantly expediting simulation time while further increasing flight fidelity.

The simulation server is based off of NASA's open-source spacecraft dynamics simulator entitled "42." 42 is a fast, comprehensive dynamics simulator written in C. A Python binding was written atop 42 so the server infrastructure could be built in a high level language, making server development much more flexible. Network requests are sent through ZMQ sockets as they are more flexible than standard sockets and have a wide array of platform and language support.

### Simulation Control Flow

When the ADC software is started in simulation mode, `libproc`, the core PolySat process library, is initialized to use "virtual time," which means the software executes as quickly as possible, ignoring any sleeps or delays, and keeps track of what the time would be if the process was executing in "normal time." Note that the user provides an initial time as a configuration value.

Simultaneously, the simulation server is started as a separate process, either on the same computer or on an external computer. The simulation server is provided with the spacecraft's initial conditions and the time that was used as the starting point in the ADC software. When the ADC software reaches its first instance of hardware interaction, a JSON packet is sent through a ZMQ socket from the ADC software to the simulation server. The packet includes the current virtual time. When the simulation server receives this packet, it commands 42 to propagate forward to the virtual time specified in the packet and performs the expected action---either modeling an actuator or responding with a sensor reading. This process repeats for as long as the simulation was configured to execute.